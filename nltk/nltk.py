# -*- coding: utf-8 -*-
"""NLTK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iruIY2yirOkfGf4DjBIWmuRmEk05sIzD

Autores: Arnaldo Gualberto e Leandro B. Marinho.
Documentação do NLTK: https://www.nltk.org/

# Tutorial NLTK 

Nesse notebooks, nós vamos aprender o básico do módulo `NLTK`(*__N__atural __L__anguage __T__ool**K**it*).

Primeiramente, vamos importar as bibliotecas python que vamos usar nesse tutorial:
"""

# O pacote stopwords possui funções para lidar com stopwords.
#
# Algoritmos de stemização (porter e snowball). 
# Porter é o mais famoso.
# Snowball permite fazer stemming em vários idiomas.
# WordNetLemmatizer permite fazer lematização.
#
# sent_tokenize faz tokenização a nível de sentença.
# word_tokenize faz tokenização a nível de palavra. 
#
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize

"""e vamos fazer o download de alguns módulos específicos do NLTK:"""

# O nltk possui vários recursos que as vezes precisamos baixar, para usar junto 
# aos recursos já importados.
#
# Aqui baixamos uma lista de stopwros (provavelmente em inglês),
# um dicionário para fazer lematização (wordnet), um modelo de aprendizado de 
# máquina, para fazer tokenização (punkt)e um modelo de aprendizagem de máquina
# para fazer POS tagging (averaged_perceptron_tagger).
#
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"""# Tokenization

**Tokenization** é o processo de transformar um texto em uma lista de tokens. Esses tokens podem ser sentenças, palavras ou símbolos.

### Sentence Tokenization
"""

# Aspas triplas são usadas para que as qubras de linha sejam capturadas.
text = """Hello Mr. Smith, how are you doing today? 
    The weather is great, and city is awesome.
    The sky is pinkish-blue. You shouldn't eat cardboard
"""

# Gerando tokens, a nível de sentença, do texto acima.
tokenized_sent = sent_tokenize(text)
print(tokenized_sent)

"""Nós também podemos tokenizar outras linguas:"""

# Aqui tokenizamos um texto, especificando uma língua (português) diferente da 
# língua default (intglês).
portuguese_text = "Bom dia, Sr. Smith. Como você está? O tempo está bom, e a cidade maravilhosa."

print(sent_tokenize(portuguese_text, "portuguese"))

"""### Word Tokenization"""

# Aqui ocorre uma tokenização a nível de palavra, que é o mais comum na maioria
# das tarefas de nlp.
tokenized_word = word_tokenize(text)
print(tokenized_word)

"""# Stopwords"""

# Criamos um conjunto (não há repetição de palvras) de stopwords, em inglês.
stop_words = set(stopwords.words("english"))

print(stop_words)

# Iteramos o conjunto de stopwords e se a palavra corrente não estiver presente
# nesse conjunto, mantemos ela. Caso contrário, descartamos.
# 
# Esse valor atribuído à variável filtered_words, está sendo usado um recurso 
# do python, chamado list comprehension.
# 
filtered_words =  [word for word in tokenized_word if word not in stop_words]

print("Tokenized Words:", tokenized_word)
print("Filterd Sentence:", filtered_words)

"""# Stemming

A **Stemming** reduz as palavras aos seus radicais. Por exemplo, as palavras *connection*, *connected*, *connecting* serão reduzidas a "*connect*". Há diversos algoritmos de stemming, mas o mais famoso é o `Porter stemming`.
"""

example_words = ['connect', 'connected', 'connecting']

ps = PorterStemmer()

# Lista com as palavras 'steemizadas'. 
stemmed_words = [ps.stem(w) for w in example_words]

print("Filtered Sentence:", example_words)
print("Stemmed Sentence:", stemmed_words)

"""O algoritmo `SnowBall` pode faz o processo de stemming em até 13 línguas diferentes:"""

print(SnowballStemmer.languages)

# Nesse techo, 'steemizamos' palavras do português.

# Nesse caso os radicais gerados, não são os mesmos para todas as palavras,
# mas deveriaam ser.Entretanto, algoritmos que usam heurísticas, estão sujeitos
# a erros.
example_words = ['conexão', 'conectado', 'conectando', 'conectar']

ss = SnowballStemmer("portuguese")

stemmed_words = [ss.stem(w) for w in example_words]

print('Stemmed sentence:', stemmed_words)

"""# Lemmatization

O processo de **Lemmatization** reduz as palavras à sua forma base, conhecida como *lemma*. Por exemplo, a palavra "better" tem "good" como sua lemma.  Em geral, é mais sofisticada que o stemming, pois leva em consideração o contexto. Entretanto, é mais lenta que o stemming.
"""

stemmer = PorterStemmer()
print(stemmer.stem('stones'))
print(stemmer.stem('speaking'))
print(stemmer.stem('are'))
print(stemmer.stem('geese'))
print(stemmer.stem('went'))

# Se uma palavra está conjugada, o algoritmo volta ela pro infinitivo.
# Se está no plural, ele volta ela para o singular.
# Podemos observar que ele é mais eficiente que o stemmer acima.
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize('stones'))
print(lemmatizer.lemmatize('speaking',pos='v'))
print(lemmatizer.lemmatize('are',pos='v'))
print(lemmatizer.lemmatize('geese'))
print(lemmatizer.lemmatize('went',pos='v'))

"""# POS Tagging

O principal objetivo de **Part-of-Speech (POS)** é identificar o grupo gramatical de uma certa palavra: *nome, pronome, adjetivo, verbo, advérbio, etc. Ela leva em consideração o contexto e procura por relacionamentos dentro da sentença e atribui uma tag correspondente a palavra.
"""

sent = "Albert Einstein was born in Ulm, Germany in 1879."

# Antes de aplicar o POS tagging, tokenizamos o texto.
# Existe uma página com todas as categorias (classes gramaticais e outras) que 
# essa lib utiliza.
# https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/
tokens = nltk.word_tokenize(sent)
print('Sentence:', tokens)

nltk.pos_tag(tokens)

"""# N-Gramas

Sequências sobrepostas de n-palavras.
"""

from nltk import bigrams
string_bigrams = list(bigrams(tokenized_word))
print(string_bigrams)

