# -*- coding: utf-8 -*-
"""Cópia de Bag of Words com SkLearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tZEmIignpwJzf8e-4XJ-lVmUxlWcsqm6
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

"""CountVectorizer tokeniza, cria o vocabulário e conta a frequência de ocorrência das palavras."""

docs=["the house had a tiny little mouse",
      "the cat saw the mouse",
      "the mouse ran away from the house",
      "the cat finally ate the mouse",
      "the end of the mouse story"
     ]
# Instancia CountVectorizer()

# Podemos setar alguns parâmetros para esse elemento, para obtermos um 
# comportamento mais adequado para determinadas tarefas.
# Veremos isso abaixo.
cv=CountVectorizer()
# CountVectorizer gera uma matriz esparsa com a contagem das 
# palavras para cada documento (baseada em Term Frequency).
#
# A matriz esparsa é útil para armazenarmos apenas a frequência das palavras que
# ocorrem no texto, sem armazenar os zeros para as palavras que não ocorrem.
# 
# Essa explicação de balby foi contraditória com a ideia de matrizesparsa, onde 
# a maioria dos elementos são zeros.
#
# Realmente esse agoritmo armazena apenas índices e valores (frequência) 
# diferentes de zero.
word_count_vector=cv.fit_transform(docs)

word_count_vector.shape

# Cada linha é um documento e cada coluna é uma palavra do vocabulário
# Obs: a dimensão dos vetores é igual ao número de palavras do vocabulário.
print(word_count_vector.toarray())

# São as palavras das colunas da matriz
  cv.get_feature_names()

"""É possível escoher o tokenizador, stopwords, n-gramas e muitos outros parâmetros."""

# Passando parâmetros para o CountVectorizer.
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer('\w+')

cv=CountVectorizer()

cv.set_params(tokenizer=tokenizer.tokenize)

# remove stop words
cv.set_params(stop_words='english')

# considera 1-gramas e 2-gramas
cv.set_params(ngram_range=(1, 2))

# ignore terms that appear in more than 50% of the documents
#vect.set_params(max_df=0.5)

# only keep terms that appear in at least 2 documents
#vect.set_params(min_df=2)

word_count_vector=cv.fit_transform(docs)

word_count_vector.shape

cv.get_feature_names()

word_count_vector.toarray()

"""TfidfVectorizer permite construir vetores TF*IDF para cada documento."""

from sklearn.feature_extraction.text import TfidfVectorizer 
 
# instancia um objeto da classe TfidfVectorizer
tfidf_vectorizer=TfidfVectorizer()
 
# passa a lista de documentos para vetorização tf-idf
tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)

tfidf_vectorizer_vectors.toarray()

